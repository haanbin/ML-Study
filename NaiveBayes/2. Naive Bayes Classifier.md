Naive Bayes Classifier
====================

- 베이즈 정리에 기반한 통계적 분류 기법
- 가장 단순한 지도 학습(supervised learning) 중 하나로 텍스트 분류를 위해 전통적으로 사용되는 분류기
- 나이브 베이즈 분류기는 빠르고, 정확하며, 믿을만한 알고리즘이며 정확성도 높고 대용량 데이터에 대해 속도도 빠르다.
- 나이브 베이즈는 feature끼리 서로 독립이라는 조건이 필요하다. 예를 들어, 스펨 메일 분류에서 광고성 단어의 개수와 비속어의 개수가 서로 연관이 있어서는 안됨

* 특징 
  * 각 특성을 개별로 취급해 파라미터를 학습하여 각 특성에서 클래스별 통계를 단순하게 취합
  * Logistic Regression, LinearSVC 같은 선형분류기보다 훈련속도가 빠름, 그러나 일반화 성능이 떨어짐

* Scikit learn에서 나이브 베이지안 분류 기준
    * MAP(Maximum A Posteriori) : 사후 확률을 계산하여 더 높은 확률을 가지는 것을 정답으로 분류

<img src="/naive1.png" width="600px" height="350px" ></img><br/>

* Scikit learn에서 제공하는 나이브 베이지안 종류 (자료의 특성에 따른 분류기 제공)
   * 가우시안 나이브 베이즈 : from sklearn.naive_bayes import GaussianNB
        * GaussianNB()
        * 연속적인 값에 사용
        * 가우시안 분포 = 정규 분포

    * 베르누이 나이브 베이즈(이항분포) : from sklearn.naive_bayes import BernoulliNB
        * BernoulliNB()
        * 이산적인 값에 사용
        * 모든 특성이 두 종류로만 나뉘는 경우

    * 다항분포 나이브 베이즈 : from sklearn.naive_bayes import MultinomialNB
        * MultimonialNB()
        * 이산적인 값에 사용 (count 데이터)
        * 하나의 특성이 여러 종류로 나뉘는 경우

* 장점
  1. 간단하고, 빠르며, 정확한 모델
  2. computation cost가 작다.
  3. 큰 데이터셋에 적합하다.
  4. 연속형보다 이산형 데이터에서 성능이 좋다.
  5. Multiple class 예측을 위해서도 사용할 수 있다.

* 단점
  1. feature 간의 독립성이 있어야 한다. 하지만 실제 데이터에서 모든 feature가 독립인 경우는 드물다. 

참고 자료
-------
- https://wikidocs.net/22892
- https://gomguard.tistory.com/69
